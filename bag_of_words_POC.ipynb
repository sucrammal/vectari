{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sucrammal/vectari/blob/main/bag_of_words_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v7u9DYN639AN",
        "outputId": "5c2c3cfa-b67f-4cd6-b8a6-025cb2895025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WxhqMJuEAQOU",
        "outputId": "f91e3fc2-1b65-4941-bc75-87c549237148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k9MuDhuL4Ess",
        "outputId": "8c60ddb2-9dcd-46a5-88d5-edb141613398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# NLTK imports\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import *\n",
        "\n",
        "# skl imports\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import string\n",
        "\n",
        "STOPWORD_SET = {'t', 'xxxx', 'XXXX', 'xx/xx/xxxx', 'xx/xx', 'their', 'should', \"doesn't\", 'under', 'wasn', 'on', 'needn', 'hadn', 'out', 'against', 'to', 'ours', 'them', 'was', 'then', 'few', 'after', 'can', 'over', \"weren't\", 'during', 'a', 'an', 'until', 'has', 'no', 'hers', 'only', \"you've\", \"aren't\", \"didn't\", 'me', 'if', 'which', \"wasn't\", 'does', 'these', 'how', 'll', 'for', 'because', 'm', 'ma', 'won', \"you'll\", 'yourselves', 'haven', 'were', 'he', 'is', 'each', 'why', 'you', 'did', 'yourself', 'from', 'been', 'more', 's', \"hasn't\", 'above', 'with', 'o', 'below', 'aren', 'it', 'now', 'ourselves', 'so', 'here', 'do', 'up', \"it's\", 'most', 'i', 'himself', 'y', 'we', 'again', 'yours', 'both', 'further', \"isn't\", 'and', 'than', 'of', 'hasn', 'into', 'or', \"should've\", 'whom', 'this', 'are', 'weren', 'what', \"wouldn't\", 're', 'she', 'herself', \"you'd\", 'same', 'having', 'by', 'where', 'they', 'off', 'about', 'shouldn', 'my', 'shan', 'as', 'isn', \"mustn't\", 'am', 'own', 'wouldn', 'those', \"haven't\", 'while', 'his', \"that'll\", 'between', 'its', 'but', 'being', 'itself', 'be', \"needn't\", 'don', 'at', 'mightn', 'doing', \"you're\", \"she's\", 'down', 'just', 'him', 'ain', 'mustn', 'theirs', 'very', \"shan't\", 'will', \"hadn't\", 'through', 'couldn', 'such', \"won't\", 'who', 'doesn', 've', 'when', 'not', 'myself', 'there', 'had', 'nor', 'other', \"don't\", 'her', \"shouldn't\", 'd', 'too', 'any', 'our', 'in', 'that', 'all', 'didn', 'themselves', 'before', 'the', \"couldn't\", 'your', \"mightn't\", 'once', 'some', 'have'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDu1cSrnENBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a445d635-b9e4-42ec-a0a2-13b7ff1e1dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://oai-vectari-northcentral-us.openai.azure.com/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Setting up API\n",
        "# gpt_config = {\n",
        "#     'model': 'gpt-4-vectari-1106preview',\n",
        "#     'azure_endpoint': 'https://oai-vectari-eastus2.openai.azure.com/',\n",
        "#     'api_key': userdata.get('openAPIKey'),\n",
        "#     'api_version': '2023-12-01-preview'\n",
        "# }\n",
        "\n",
        "gpt_config = {\n",
        "    'model': 'gpt-35-turbo',\n",
        "    'azure_endpoint': 'https://oai-vectari-northcentral-us.openai.azure.com/',\n",
        "    'api_key': userdata.get('openAPIKey3.5'),\n",
        "    'api_version': '2024-02-15-preview'\n",
        "}\n",
        "\n",
        "print(gpt_config['azure_endpoint'])\n",
        "\n",
        "client = AzureOpenAI(\n",
        "  azure_endpoint = gpt_config['azure_endpoint'],\n",
        "  api_key = gpt_config['api_key'],\n",
        "  api_version = gpt_config['api_version']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlN684Hyrei1"
      },
      "source": [
        "# **Data Preprocessing:**\n",
        "I'll be using chatGPT to clean up transcripts, and then use some classic NLP techniques to clean the sentences by tokenizing them, getting rid of stopwords, and stem the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu4lKQ28Gd3d"
      },
      "outputs": [],
      "source": [
        "# gpt prompt to clean up and summarize the transcript.\n",
        "\n",
        "system = \"\"\"You will receive a transcript of a phone call between a call center agent at a mortgage servicing company and a customer/borrower.\n",
        "The transcription text is also low quality. These are calls about financial services and we have seen some text completely out of place.  For instance 'the first drug was fixed, and any drug after that was gonna be viral' should pretty clearly be\n",
        "'the first rate was fixed, and any rate after that was going to be variable'.\n",
        "Your job is to parse the text and using your natural language understanding and contextual awareness, please re-write the transcript and clean any grammar / things that do not make sense in financial use cases. After the response is generated, remove all full stops, commas, semicolons, colons, and quotation marks.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n0drfjXY4GJJ"
      },
      "outputs": [],
      "source": [
        "# instantiate input file\n",
        "all_complaints = pd.read_csv(\"/content/drive/MyDrive/Internships and Work/Summer 2024/Vectari/data/money_transfer_complaints.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_5y70fh64nMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cc47d7-749f-4690-8cd5-5d5a1e5ad28f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1497"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# extract complaints with transcripts\n",
        "complaints_with_transcripts = all_complaints.loc[all_complaints[\"Consumer complaint narrative\"].notna()]\n",
        "complaints_with_transcripts.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDVUd4kwJWvf"
      },
      "outputs": [],
      "source": [
        "LLM_complaints = complaints_with_transcripts[['Issue']].copy()\n",
        "LLM_complaints['Consumer complaint narrative'] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process one input as a test trial**"
      ],
      "metadata": {
        "id": "vnsfSm6sA5Qx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VQ739xsV9rC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bb2c38-9ad0-4b18-f05b-cbad514997ed",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I received a phone call from someone claiming that I had been selected to receive a government grant. They said I could receive the grant through Western Union. All I had to do was send a deposit to hold the money, and I would get the deposit back with the grant. So, I sent the money and now I'm waiting to receive a tracking number. However, they told me to call another number for the Federal Reserve Bank. When I called them, they asked me to send more money. I refused and asked for a refund of the money I already sent. They told me they can't refund it because it has already been processed. I don't know what to do now to get my money back.\n"
          ]
        }
      ],
      "source": [
        "# test: LLM cleaning on 1 transcript\n",
        "input_text = complaints_with_transcripts.iloc[0][\"Consumer complaint narrative\"]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\": system\n",
        "    },\n",
        "    {\n",
        "        \"role\":\"user\",\n",
        "        \"content\":f'\"\"\"{input_text}\"\"\"'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=gpt_config['model'],\n",
        "    messages = messages\n",
        ")\n",
        "\n",
        "LLM_cleaned_sample = response.choices[0].message.content\n",
        "print(LLM_cleaned_sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation and lowercase.\n",
        "for punctuation in string.punctuation:\n",
        "    LLM_cleaned_sample = LLM_cleaned_sample.replace(punctuation, '')\n",
        "\n",
        "print(LLM_cleaned_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G8DV6ZiDRKg",
        "outputId": "5b4125ed-ebbd-49d4-8f05-ed3a85174ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I received a phone call from someone claiming that I had been selected to receive a government grant They said I could receive the grant through Western Union All I had to do was send a deposit to hold the money and I would get the deposit back with the grant So I sent the money and now Im waiting to receive a tracking number However they told me to call another number for the Federal Reserve Bank When I called them they asked me to send more money I refused and asked for a refund of the money I already sent They told me they cant refund it because it has already been processed I dont know what to do now to get my money back\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, tokenize sentence\n",
        "tokenized_sample = word_tokenize(LLM_cleaned_sample)\n",
        "print(tokenized_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoifPggFBoCE",
        "outputId": "1d76a690-d788-4081-fd1c-131f2854826e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'received', 'a', 'phone', 'call', 'from', 'someone', 'claiming', 'that', 'I', 'had', 'been', 'selected', 'to', 'receive', 'a', 'government', 'grant', 'They', 'said', 'I', 'could', 'receive', 'the', 'grant', 'through', 'Western', 'Union', 'All', 'I', 'had', 'to', 'do', 'was', 'send', 'a', 'deposit', 'to', 'hold', 'the', 'money', 'and', 'I', 'would', 'get', 'the', 'deposit', 'back', 'with', 'the', 'grant', 'So', 'I', 'sent', 'the', 'money', 'and', 'now', 'Im', 'waiting', 'to', 'receive', 'a', 'tracking', 'number', 'However', 'they', 'told', 'me', 'to', 'call', 'another', 'number', 'for', 'the', 'Federal', 'Reserve', 'Bank', 'When', 'I', 'called', 'them', 'they', 'asked', 'me', 'to', 'send', 'more', 'money', 'I', 'refused', 'and', 'asked', 'for', 'a', 'refund', 'of', 'the', 'money', 'I', 'already', 'sent', 'They', 'told', 'me', 'they', 'cant', 'refund', 'it', 'because', 'it', 'has', 'already', 'been', 'processed', 'I', 'dont', 'know', 'what', 'to', 'do', 'now', 'to', 'get', 'my', 'money', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and stem words\n",
        "stemmer = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "removed_stopwords_sample = []\n",
        "for word in tokenized_sample:\n",
        "  if word not in STOPWORD_SET:\n",
        "    word = word.lower()\n",
        "    # Sometimes, stemming a word ending in 'e' will just get rid of the e. Here is a quick but rough fix\n",
        "    wnl.lemmatize(word) if wnl.lemmatize(word).endswith('e') else stemmer.stem(word)\n",
        "    removed_stopwords_sample.append(word)\n",
        "print(removed_stopwords_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv_-C6vmBKJ4",
        "outputId": "6a1b6169-e799-4924-d3e8-2de6bb68aa81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'received', 'phone', 'call', 'someone', 'claiming', 'i', 'selected', 'receive', 'government', 'grant', 'they', 'said', 'i', 'could', 'receive', 'grant', 'western', 'union', 'all', 'i', 'send', 'deposit', 'hold', 'money', 'i', 'would', 'get', 'deposit', 'back', 'grant', 'so', 'i', 'sent', 'money', 'im', 'waiting', 'receive', 'tracking', 'number', 'however', 'told', 'call', 'another', 'number', 'federal', 'reserve', 'bank', 'when', 'i', 'called', 'asked', 'send', 'money', 'i', 'refused', 'asked', 'refund', 'money', 'i', 'already', 'sent', 'they', 'told', 'cant', 'refund', 'already', 'processed', 'i', 'dont', 'know', 'get', 'money', 'back']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LLM_cleanup(transcript):\n",
        "  messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\": system\n",
        "    },\n",
        "    {\n",
        "        \"role\":\"user\",\n",
        "        \"content\":f'\"\"\"{transcript}\"\"\"'\n",
        "    }\n",
        "  ]\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=gpt_config['model'],\n",
        "      messages = messages\n",
        "  )\n",
        "\n",
        "  LLM_cleaned_sample = response.choices[0].message.content\n",
        "  return LLM_cleaned_sample"
      ],
      "metadata": {
        "id": "qyxPHPSVZ7zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NL_cleanup(LLM_cleaned_sample):\n",
        "  # remove punctuation\n",
        "  for punctuation in string.punctuation:\n",
        "    LLM_cleaned_sample = LLM_cleaned_sample.replace(punctuation, '')\n",
        "\n",
        "  # tokenize\n",
        "  tokenized_sample = word_tokenize(LLM_cleaned_sample)\n",
        "\n",
        "  # Remove stopwords and stem words\n",
        "  stemmer = PorterStemmer()\n",
        "  wnl = WordNetLemmatizer()\n",
        "\n",
        "  removed_stopwords_sample = []\n",
        "  for word in tokenized_sample:\n",
        "    if word not in STOPWORD_SET:\n",
        "      word = word.lower()\n",
        "      # Sometimes, stemming a word ending in 'e' will just get rid of the e. Here is a quick but rough fix\n",
        "      wnl.lemmatize(word) if wnl.lemmatize(word).endswith('e') else stemmer.stem(word)\n",
        "      removed_stopwords_sample.append(word)\n",
        "\n",
        "  cleaned_sentence = \" \".join(removed_stopwords_sample)\n",
        "\n",
        "  return cleaned_sentence\n"
      ],
      "metadata": {
        "id": "2VkKsZ-abQPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# use the written functions!\n",
        "cleaned_sample_1 = NL_cleanup(LLM_cleanup(complaints_with_transcripts.iloc[0][\"Consumer complaint narrative\"]))\n",
        "cleaned_sample_2 = NL_cleanup(LLM_cleanup(complaints_with_transcripts.iloc[1][\"Consumer complaint narrative\"]))"
      ],
      "metadata": {
        "id": "G90EB23ZdrS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_sample = [cleaned_sample_1, cleaned_sample_2]\n",
        "train_label_sample = [complaints_with_transcripts.iloc[0][\"Issue\"], complaints_with_transcripts.iloc[1][\"Issue\"]]"
      ],
      "metadata": {
        "id": "FM3SaZ0tfLSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_text_sample)\n",
        "train_vect = vectorizer.transform(train_text_sample)\n",
        "print(train_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3LdSU44uEyrL",
        "outputId": "6057634b-3916-4dc2-ecf7-8a21e3e392cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 7)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 21)\t2\n",
            "  (0, 23)\t1\n",
            "  (0, 30)\t2\n",
            "  (0, 32)\t1\n",
            "  (0, 41)\t1\n",
            "  (0, 44)\t1\n",
            "  (0, 55)\t1\n",
            "  (0, 60)\t3\n",
            "  (0, 75)\t1\n",
            "  (0, 78)\t1\n",
            "  (0, 85)\t1\n",
            "  (0, 86)\t1\n",
            "  (0, 87)\t2\n",
            "  (0, 89)\t1\n",
            "  (0, 92)\t1\n",
            "  (0, 101)\t1\n",
            "  (0, 106)\t1\n",
            "  (0, 116)\t4\n",
            "  (0, 121)\t1\n",
            "  (0, 123)\t2\n",
            "  :\t:\n",
            "  (1, 201)\t1\n",
            "  (1, 202)\t1\n",
            "  (1, 203)\t1\n",
            "  (1, 205)\t3\n",
            "  (1, 206)\t3\n",
            "  (1, 207)\t3\n",
            "  (1, 208)\t2\n",
            "  (1, 209)\t1\n",
            "  (1, 210)\t1\n",
            "  (1, 211)\t1\n",
            "  (1, 212)\t1\n",
            "  (1, 215)\t2\n",
            "  (1, 216)\t6\n",
            "  (1, 217)\t1\n",
            "  (1, 218)\t1\n",
            "  (1, 219)\t1\n",
            "  (1, 221)\t1\n",
            "  (1, 222)\t1\n",
            "  (1, 223)\t13\n",
            "  (1, 226)\t3\n",
            "  (1, 227)\t1\n",
            "  (1, 228)\t1\n",
            "  (1, 229)\t2\n",
            "  (1, 230)\t2\n",
            "  (1, 231)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now process ALL transcripts**"
      ],
      "metadata": {
        "id": "6OeMXQFQEsmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = []\n",
        "all_labels = complaints_with_transcripts['Issue']\n",
        "\n",
        "for index in range(complaints_with_transcripts.shape[0]):\n",
        "  cleaned_sentences.append(NL_cleanup(LLM_cleanup(complaints_with_transcripts.iloc[index][\"Consumer complaint narrative\"])))\n",
        "\n"
      ],
      "metadata": {
        "id": "x1nRzE7thO38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Wg_JGH2tIa0b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown  Following hiddencode blocks are hard-coded, unfactored trials.\n",
        "# use chatGPT to clean up all transcripts.\n",
        "# use LLM get summarized and cleaned up transcript.\n",
        "for idx in range(complaints_with_transcripts.shape[0]):\n",
        "  input_text = complaints_with_transcripts.iloc[idx][\"Consumer complaint narrative\"]\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\":\"system\",\n",
        "          \"content\": system\n",
        "      },\n",
        "      {\n",
        "          \"role\":\"user\",\n",
        "          \"content\":f'\"\"\"{input_text}\"\"\"'\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=gpt_config['model'],\n",
        "      messages = messages\n",
        "  )\n",
        "\n",
        "  LLM_complaints['Consumer complaint narrative'][idx] = response.choices[0].message.content\n",
        "\n",
        "cleaned_complaints = complaints_with_transcripts[['Issue']].copy()\n",
        "cleaned_complaints['Consumer complaint narrative'] = \"\"\n",
        "\n",
        "# tokenize, stem, and remove stopwords from the transcripts\n",
        "# cleaned_complaints = complaints_with_transcripts[[\"Issue\", \"Consumer complaint narrative\"]]\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "for i in range(LLM_complaints.shape[0]):\n",
        "  current_transcript = LLM_complaints.iloc[i][\"Consumer complaint narrative\"]\n",
        "  tokenized = word_tokenize(current_transcript)\n",
        "  cleaned = []\n",
        "  for j in range(len(tokenized)):\n",
        "    if tokenized[j] not in STOPWORD_SET:\n",
        "      cleaned.append(stemmer.stem(tokenized[j]))\n",
        "\n",
        "  cleaned_complaints[\"Consumer complaint narrative\"][i] = cleaned\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFiCAzx44rhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c41019e6-d294-4a2b-e1a3-83daba8e5005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of entries under each 'Issue' type:\n",
            "Issue\n",
            "Fraud or scam                            508\n",
            "Other transaction issues                 469\n",
            "Money was not available when promised    259\n",
            "Other service issues                     130\n",
            "Wrong amount charged or received          73\n",
            "Incorrect/missing disclosures or info     58\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# aggregate data via \"Issue\" type to build bag of words for each category.\n",
        "issue_groups = cleaned_complaints.groupby(\"Issue\")\n",
        "\n",
        "# identify unique \"Issue\" labels\n",
        "issue_counts = cleaned_complaints['Issue'].value_counts()\n",
        "print(\"\\nNumber of entries under each 'Issue' type:\")\n",
        "print(issue_counts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_sentences, train_labels):\n",
        "  vectorizer = CountVectorizer()\n",
        "  vectorizer.fit(train_sentences)\n",
        "  train_vect = vectorizer.transform(train_sentences)\n",
        "  print(train_vect)\n",
        "  model = LogisticRegression()\n",
        "  model.fit(train_vect, train_labels)\n",
        "  return model, vectorizer\n",
        "\n",
        "\n",
        "def predict(test_sentences, test_labels, vectorizer, model):\n",
        "  test_sentences = [\" \".join(t) for t in  test_sentences]\n",
        "  test_vect = vectorizer.transform(test_sentences)\n",
        "  preds = model.predict(test_vect)\n",
        "  acc = accuracy_score(test_labels, preds)\n",
        "  return preds, acc\n"
      ],
      "metadata": {
        "id": "TWTPNOwQfqo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsfHdLV1r6SH"
      },
      "source": [
        "# **Train test split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga5sc3vS36LG"
      },
      "outputs": [],
      "source": [
        "# vectorize cleaned transcripts\n",
        "vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mvNGX9RpNoI"
      },
      "outputs": [],
      "source": [
        "# train test split. X are features, y are labels\n",
        "X = cleaned_complaints['Consumer complaint narrative']\n",
        "y = cleaned_complaints['Issue']\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test = train_test_split(X, y ,random_state=104, test_size=0.25, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03Voc0xnvPER"
      },
      "source": [
        "**Note on transform:**  This step converts each sentence into a vector of token counts based on the vocabulary learned during the fit step. The result is a sparse matrix where each row represents a sentence and each column represents a token from the vocabulary. The values in the matrix are the counts of each token in the corresponding sentence. This will be useful when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNjh6RzXmwSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a175c85e-8ea6-47c1-aff2-ad0e3798ec69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (5, 0)\t4\n",
            "  (5, 23)\t1\n",
            "  (5, 34)\t3\n",
            "  (5, 48)\t7\n",
            "  (5, 53)\t1\n",
            "  (5, 59)\t1\n",
            "  (5, 73)\t1\n",
            "  (5, 76)\t1\n",
            "  (5, 81)\t2\n",
            "  (5, 87)\t2\n",
            "  (5, 89)\t1\n",
            "  (5, 97)\t1\n",
            "  (5, 100)\t1\n",
            "  (5, 102)\t1\n",
            "  (5, 117)\t1\n",
            "  (5, 135)\t1\n",
            "  (5, 152)\t1\n",
            "  (5, 163)\t1\n",
            "  (5, 179)\t1\n",
            "  (5, 213)\t1\n",
            "  (5, 216)\t1\n",
            "  (5, 223)\t1\n",
            "  (5, 226)\t6\n",
            "  (5, 234)\t1\n",
            "  (5, 240)\t1\n",
            "  :\t:\n",
            "  (1121, 666)\t3\n",
            "  (1121, 673)\t1\n",
            "  (1121, 681)\t1\n",
            "  (1121, 707)\t1\n",
            "  (1121, 734)\t1\n",
            "  (1121, 741)\t4\n",
            "  (1121, 751)\t1\n",
            "  (1121, 775)\t1\n",
            "  (1121, 776)\t1\n",
            "  (1121, 785)\t1\n",
            "  (1121, 787)\t2\n",
            "  (1121, 810)\t1\n",
            "  (1121, 898)\t1\n",
            "  (1121, 905)\t1\n",
            "  (1121, 911)\t1\n",
            "  (1121, 934)\t1\n",
            "  (1121, 939)\t1\n",
            "  (1121, 946)\t1\n",
            "  (1121, 955)\t3\n",
            "  (1121, 982)\t1\n",
            "  (1121, 993)\t1\n",
            "  (1121, 999)\t1\n",
            "  (1121, 1029)\t5\n",
            "  (1121, 1041)\t1\n",
            "  (1121, 1049)\t2\n"
          ]
        }
      ],
      "source": [
        "# Vectorize training data\n",
        "train_sentences = [\" \".join(t) for t in train_sentences]\n",
        "train_labels = [l for l in train_labels]\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_sentences)\n",
        "train_vect = vectorizer.transform(train_sentences)\n",
        "print(train_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tB-aWMSvthQ"
      },
      "outputs": [],
      "source": [
        "# Combine into function to do Logistic regression training\n",
        "def train_model(train_sentences, train_labels):\n",
        "  train_sentences = [\" \".join(t) for t in train_sentences]\n",
        "  train_labels = [l for l in train_labels]\n",
        "  vectorizer = CountVectorizer()\n",
        "  vectorizer.fit(train_sentences)\n",
        "  train_vect = vectorizer.transform(train_sentences)\n",
        "  print(train_vect)\n",
        "  model = LogisticRegression()\n",
        "  model.fit(train_vect, train_labels)\n",
        "  return model, vectorizer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}